{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Start training run 0!\n",
      "Training model with 2531 parameters...\n",
      "it 0: avgR: 20.732 -- rollout_time: 1.225sec -- update_time: 1.036sec\n",
      "it 1: avgR: 20.111 -- rollout_time: 1.289sec -- update_time: 0.826sec\n",
      "it 2: avgR: 21.274 -- rollout_time: 1.322sec -- update_time: 0.798sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-73c905f774bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mppo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start training run {}!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msuccess_rates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All training runs completed!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-73c905f774bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    195\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting_iters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                                 \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_policy_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                 \u001b[0;31m# render learning logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# this removes all training outputs to keep the notebook clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-73c905f774bd>\u001b[0m in \u001b[0;36mlog_policy_rollout\u001b[0;34m(self, policy, env_name, pytorch_policy)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m~/tool/miniconda3/envs/gail/lib/python3.7/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "import utils\n",
    "from utils import show_video\n",
    "from utils import count_model_params\n",
    "from utils import AverageMeter, plot_learning_curve\n",
    "from utils import ParamDict\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "class PPO():\n",
    "\tdef __init__(self, params_in):\n",
    "\t\tself.params = params_in\n",
    "\t\t# env\n",
    "\t\tself.env = self.gen_env()\n",
    "\t\tself.obs_size = self.env.observation_space.shape[0]\n",
    "\t\tself.num_actions = self.env.action_space.n\n",
    "\t\t# rollouts\n",
    "\t\tself.rollouts = self.RolloutStorage(self.params.rollout_size, self.obs_size)\n",
    "\t\t# policy\n",
    "\t\tself.init_policy(self.obs_size, self.num_actions, **self.params.policy_params)\n",
    "\t\n",
    "\tdef init_policy(self, num_inputs, num_actions, hidden_dim, learning_rate, batch_size, policy_epochs,\n",
    "\t\t\t\tentropy_coef=0.001, critic_coef=0.5, clip_param=0.2):\n",
    "\t\tself.actor = self.ActorNetwork(num_inputs, num_actions, hidden_dim)\n",
    "\t\tself.critic = self.CriticNetwork(num_inputs, hidden_dim)\n",
    "\t\tself.critic_coef = critic_coef\n",
    "\t\tself.optimizer = optim.Adam( (list(self.actor.parameters()) + list(self.critic.parameters())), lr=learning_rate)\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.policy_epochs = policy_epochs\n",
    "\t\tself.entropy_coef = entropy_coef\n",
    "\t\tself.clip_param = clip_param\n",
    "\n",
    "\tdef gen_env(self):\n",
    "\t\t# env = Monitor(gym.make(self.params.env_name), './video', force=True)\n",
    "\t\t# env = self.FlatObsWrapperCartPole(gym.make(self.params.env_name))\n",
    "\t\tenv = gym.make(self.params.env_name)\n",
    "\t\treturn env\n",
    "\n",
    "\tdef act(self, state):\n",
    "\t\tlogits = self.actor.forward(state)\n",
    "\t\t# print(\"logits: {}\".format(logits))\n",
    "\t\tdist = Categorical(logits = logits)\n",
    "\t\t# print(\"dist: {}\".format(dist))\n",
    "\t\taction = dist.sample()\n",
    "\t\t# print(\"action: {}\".format(action))\n",
    "\t\tlog_prob = dist.log_prob(action)\n",
    "\t\t# print(\"log_prob: {}\".format(log_prob))\n",
    "\t\treturn action, log_prob\n",
    "\t\n",
    "\tdef update(self, rollouts):\t\n",
    "\t\tfor epoch in range(self.policy_epochs):\n",
    "\t\t\tdata = rollouts.batch_sampler(self.batch_size, get_old_log_probs=True)\n",
    "\t\t\tfor sample in data:\n",
    "\t\t\t\tactions_batch, returns_batch, obs_batch, old_log_probs_batch = sample\n",
    "\t\t\t\tlog_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
    "\t\t\t\t\n",
    "\t\t\t\tvalue_batch = self.critic(obs_batch)\n",
    "\t\t\t\t\n",
    "\t\t\t\tadvantage = returns_batch - value_batch.detach()\n",
    "\t\t\t\told_log_probs_batch = old_log_probs_batch.detach()\n",
    "\n",
    "\t\t\t\tratio = torch.exp(log_probs_batch-old_log_probs_batch)\n",
    "\t\t\t\tsurr1 = ratio * advantage\n",
    "\t\t\t\tsurr2 = torch.clamp(ratio,1-self.clip_param, 1+ self.clip_param) * advantage\n",
    "\n",
    "\t\t\t\tpolicy_loss = -1 * (torch.min(surr1,surr2).mean())\n",
    "\t\t\t\tentropy_loss = -1 * ((entropy_batch).mean())\n",
    "\t\t\t\tloss = torch.nn.MSELoss()\n",
    "\t\t\t\tcritic_loss = loss(returns_batch,value_batch)   \n",
    "\t\t\t\tloss = policy_loss + \\\n",
    "\t\t\t\t\t\tself.critic_coef * critic_loss + \\\n",
    "\t\t\t\t\t\tself.entropy_coef * entropy_loss\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss.backward(retain_graph=False)\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\t\n",
    "\tdef evaluate_actions(self, state, action):\n",
    "\t\tlogits = self.actor.forward(state)\n",
    "\t\tdist = Categorical(logits = logits)\n",
    "\t\tlog_prob = dist.log_prob(action.squeeze(-1)).view(-1, 1)  # ???\n",
    "\t\tentropy = dist.entropy().view(-1, 1)\n",
    "\t\treturn log_prob, entropy\n",
    "\n",
    "\tdef log_policy_rollout(self, policy, env_name, pytorch_policy=False):\n",
    "\t\tenv = Monitor(gym.make(env_name), './video', force=True)\n",
    "\t\t# env = gym.make(env_name)\n",
    "\t\t# env.monitor.start('./video', force=True)\n",
    "\n",
    "\t\tdone = False\n",
    "\t\tepisode_reward = 0\n",
    "\t\tepisode_length = 0\n",
    "\t\tobservation = env.reset()\n",
    "\n",
    "\t\twhile not done:\n",
    "\t\t\tif pytorch_policy: \n",
    "\t\t\t\tobservation = torch.tensor(observation, dtype=torch.float32)\n",
    "\t\t\t\taction = policy.act(observation)[0].data.cpu().numpy()\n",
    "\t\t\telse:\n",
    "\t\t\t\taction = policy.act(observation)[0]\n",
    "\t\t\tobservation, reward, done, info = env.step(action)\n",
    "\n",
    "\t\t\tepisode_reward += reward\n",
    "\t\t\tepisode_length += 1\n",
    "\n",
    "\t\tprint('Total reward:', episode_reward)\n",
    "\t\tprint('Total length:', episode_length)\n",
    "\n",
    "\t\tenv.close()\n",
    "\t\t# env.monitor.close()\n",
    "\t\tshow_video()\n",
    "\t\n",
    "\tdef train(self, seed=123):\n",
    "\t\t# SETTING SEED: it is good practice to set seeds when running experiments to keep results comparable\n",
    "\t\tnp.random.seed(seed)\n",
    "\t\ttorch.manual_seed(seed)\n",
    "\t\tself.env.seed(seed)\n",
    "\n",
    "\t\trollout_time, update_time = AverageMeter(), AverageMeter()  # Loggers\n",
    "\t\trewards, success_rate = [], []\n",
    "\n",
    "\t\tprint(\"Training model with {} parameters...\".format(self.num_params))\n",
    "\n",
    "\t\tfor j in range(self.params.num_updates):\n",
    "\t\t\t## initialization\n",
    "\t\t\tavg_eps_reward, avg_success_rate = AverageMeter(), AverageMeter()\n",
    "\t\t\tdone = False\n",
    "\t\t\tprev_obs = self.env.reset()\n",
    "\t\t\t# print(prev_obs)\n",
    "\t\t\tprev_obs = torch.tensor(prev_obs, dtype=torch.float32)\n",
    "\t\t\teps_reward = 0.\n",
    "\t\t\tstart_time = time.time()\n",
    "\t\t\t\n",
    "\t\t\t## collect rollouts\n",
    "\t\t\tfor step in range(self.rollouts.rollout_size):\n",
    "\t\t\t\tif done:\n",
    "\t\t\t\t\t# store episode statistics\n",
    "\t\t\t\t\tavg_eps_reward.update(eps_reward)\n",
    "\t\t\t\t\tif 'success' in info: \n",
    "\t\t\t\t\t\tavg_success_rate.update(int(info['success']))\n",
    "\n",
    "\t\t\t\t\t# reset Environment\n",
    "\t\t\t\t\tobs = self.env.reset()\n",
    "\t\t\t\t\tobs = torch.tensor(obs, dtype=torch.float32)\n",
    "\t\t\t\t\teps_reward = 0.\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tobs = prev_obs\n",
    "\n",
    "\t\t\t\taction, log_prob = self.act(obs)\n",
    "\t\t\t\tobs, reward, done, info = self.env.step(action.item())\n",
    "\t\t\t\tself.rollouts.insert(step, torch.tensor(done, dtype=torch.float32),\\\n",
    "\t\t\t\t\t\t\t\taction, log_prob, torch.tensor(reward, dtype=torch.float32), prev_obs[0])\n",
    "\t\t\t\t\n",
    "\t\t\t\tprev_obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\t\t\t\teps_reward += reward\n",
    "\t\t\t\n",
    "\t\t\tself.rollouts.compute_returns(self.params.discount)\n",
    "\t\t\trollout_done_time = time.time()\n",
    "\t\t\tself.update(self.rollouts)\n",
    "\n",
    "\t\t\tupdate_done_time = time.time()\n",
    "\t\t\tself.rollouts.reset()\n",
    "\n",
    "\t\t\t## log metrics\n",
    "\t\t\trewards.append(avg_eps_reward.avg)\n",
    "\t\t\tif avg_success_rate.count > 0:\n",
    "\t\t\t\tsuccess_rate.append(avg_success_rate.avg)\n",
    "\t\t\trollout_time.update(rollout_done_time - start_time)\n",
    "\t\t\tupdate_time.update(update_done_time - rollout_done_time)\n",
    "\t\t\tprint('it {}: avgR: {:.3f} -- rollout_time: {:.3f}sec -- update_time: {:.3f}sec'.format(j, avg_eps_reward.avg, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trollout_time.avg, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tupdate_time.avg))\n",
    "\t\t\tif j % self.params.plotting_iters == 0 and j != 0:\n",
    "\t\t\t\tplot_learning_curve(rewards, success_rate, self.params.num_updates)\n",
    "\t\t\t\tself.log_policy_rollout(self, self.params.env_name, True)\n",
    "\t\t# render learning logger\n",
    "\t\tclear_output()   # this removes all training outputs to keep the notebook clean\n",
    "\t\t# plot_learning_curve(rewards, success_rate, params.num_updates)\n",
    "\t\t# for _ in range(3):\n",
    "\t\t# \tself.log_policy_rollout(self, self.params.env_name, True)\n",
    "\t\treturn rewards, success_rate\n",
    "\t\t\n",
    "\t@property\n",
    "\tdef num_params(self):\n",
    "\t\treturn count_model_params(self.actor) + count_model_params(self.critic)\n",
    "\t\n",
    "\n",
    "\tclass FlatObsWrapperCartPole(gym.core.ObservationWrapper):\n",
    "\t\t\"\"\"Fully observable gridworld returning a flat grid encoding.\"\"\"\n",
    "\t\tdef __init__(self, env):\n",
    "\t\t\tsuper().__init__(env)\n",
    "\t\t\tself.observation_space = spaces.Box(\n",
    "\t\t\t\tlow=0,\n",
    "\t\t\t\thigh=255,\n",
    "\t\t\t\tshape=((env.observation_space['image'].shape[0]) * (env.observation_space['image'].shape[1]) * 3,),  # number of cells\n",
    "\t\t\t\tdtype='uint8'\n",
    "\t\t\t)\n",
    "\t\t\tself.unwrapped.max_steps = env.max_steps\n",
    "\n",
    "\t\tdef observation(self, obs):\n",
    "\t\t\t# this method is called in the step() function to get the observation\n",
    "\t\t\t# we provide code that gets the grid state and places the agent in it\n",
    "\t\t\tenv = self.unwrapped\n",
    "\t\t\tfull_grid = env.grid.encode()\n",
    "\t\t\t# full_grid[env.agent_pos[0]][env.agent_pos[1]] = np.array([\n",
    "\t\t\t# \tOBJECT_TO_IDX['agent'],\n",
    "\t\t\t# \tCOLOR_TO_IDX['red'],\n",
    "\t\t\t# \tenv.agent_dir\n",
    "\t\t\t# ])\n",
    "\t\t\t# full_grid = full_grid[1:-1, 1:-1]   # remove outer walls of the environment (for efficiency)\n",
    "\t\t\tflattened_grid = full_grid.reshape((np.prod(full_grid.shape),1))\n",
    "\t\t\treturn np.transpose(flattened_grid)\n",
    "\t\t\n",
    "\t\tdef render(self, *args, **kwargs):\n",
    "\t\t\tkwargs['highlight'] = False\n",
    "\t\t\treturn self.unwrapped.render(*args, **kwargs)\n",
    "\n",
    "\tclass ActorNetwork(nn.Module):\n",
    "\t\tdef __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.num_actions = num_actions\n",
    "\t\t\tself.fc = nn.Sequential(nn.Linear(num_inputs,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim, self.num_actions))\n",
    "\t\t\t\t\t\t\t\t\t# no activation in NN, output logit\n",
    "\t\t\n",
    "\t\tdef forward(self, state):\n",
    "\t\t\tx = self.fc(state)\n",
    "\t\t\treturn x\n",
    "\t\n",
    "\tclass CriticNetwork(nn.Module):\n",
    "\t\tdef __init__(self, num_inputs, hidden_dim):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.fc = nn.Sequential(nn.Linear(num_inputs,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim,1))\n",
    "\n",
    "\t\tdef forward(self, state):\n",
    "\t\t\tx = self.fc(state)\n",
    "\t\t\treturn x\n",
    "\t\n",
    "\tclass RolloutStorage():\n",
    "\t\tdef __init__(self, rollout_size, obs_size):\n",
    "\t\t\tself.rollout_size = rollout_size\n",
    "\t\t\tself.obs_size = obs_size\n",
    "\t\t\tself.reset()\n",
    "\n",
    "\t\tdef insert(self, step, done, action, log_prob, reward, obs):\t\n",
    "\t\t\tself.done[step].copy_(done)\n",
    "\t\t\tself.actions[step].copy_(action)\n",
    "\t\t\tself.log_probs[step].copy_(log_prob)\n",
    "\t\t\tself.rewards[step].copy_(reward)\n",
    "\t\t\tself.obs[step].copy_(obs)\n",
    "\n",
    "\t\tdef reset(self):\n",
    "\t\t\tself.done = torch.zeros(self.rollout_size, 1)\n",
    "\t\t\tself.returns = torch.zeros(self.rollout_size+1, 1, requires_grad=False)\n",
    "\t\t\tself.actions = torch.zeros(self.rollout_size, 1, dtype=torch.int64)\n",
    "\t\t\tself.log_probs = torch.zeros(self.rollout_size, 1)\n",
    "\t\t\tself.rewards = torch.zeros(self.rollout_size, 1)\n",
    "\t\t\tself.obs = torch.zeros(self.rollout_size, self.obs_size)\n",
    "\n",
    "\t\tdef compute_returns(self, gamma):\n",
    "\t\t\tself.last_done = (self.done == 1).nonzero().max()\n",
    "\t\t\tself.returns[self.last_done+1] = 0.  # ???\n",
    "\n",
    "\t\t\t#  compute finite-discoiunted return\n",
    "\t\t\tfor step in reversed(range(self.last_done+1)):\n",
    "\t\t\t\tself.returns[step] = self.returns[step + 1] * \\\n",
    "\t\t\t\t\tgamma * (1 - self.done[step]) + self.rewards[step]\n",
    "\n",
    "\t\tdef batch_sampler(self, batch_size, get_old_log_probs=False):\n",
    "\t\t\t# build a generator of single-trajectory: (a, r, s, pi(a, t)) \n",
    "\t\t\tsampler = BatchSampler(\n",
    "\t\t\t\tSubsetRandomSampler(range(self.last_done)),\n",
    "\t\t\t\tbatch_size,\n",
    "\t\t\t\tdrop_last=True)\n",
    "\t\t\tfor indices in sampler:\n",
    "\t\t\t\tif get_old_log_probs:\n",
    "\t\t\t\t\tyield self.actions[indices], self.returns[indices], self.obs[indices], self.log_probs[indices]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tyield self.actions[indices], self.returns[indices], self.obs[indices]\n",
    "\n",
    "# Test Trian\n",
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "\t# policy_class = PPO,  # Policy class to use (replaced later)\n",
    "\thidden_dim = 32,\t\t  # dimension of the hidden state in actor network\n",
    "\tlearning_rate = 1e-3,\t # learning rate of policy update\n",
    "\tbatch_size = 1024,\t\t# batch size for policy update\n",
    "\tpolicy_epochs = 4,\t\t# number of epochs per policy update\n",
    "\tentropy_coef = 0.001,\t # hyperparameter to vary the contribution of entropy loss\n",
    "\tcritic_coef = 0.5\t\t # Coefficient of critic loss when weighted against actor loss\n",
    ")\n",
    "params = ParamDict(\n",
    "\tpolicy_params = policy_params,\n",
    "\trollout_size = 2050,\t  # number of collected rollout steps per policy update\n",
    "\tnum_updates = 6,\t\t# number of training policy iterations\n",
    "\tdiscount = 0.99,\t\t  # discount factor\n",
    "\tplotting_iters = 2,\t  # interval for logging graphs and policy rollouts\n",
    "\tenv_name = 'CartPole-v1',  # we are using a tiny environment here for testing\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ppo = PPO(params)\n",
    "# ppo.train()\n",
    "# print(\"Training completed!\")\n",
    "\n",
    "n_seeds = 3\n",
    "rewards, success_rates = [], []\n",
    "for i in range(n_seeds):\n",
    "\tppo = PPO(params)\n",
    "\tprint(\"Start training run {}!\".format(i))\n",
    "\tr, sr = ppo.train(i)\n",
    "\trewards.append(r); success_rates.append(sr)\n",
    "print('All training runs completed!')\n",
    "\n",
    "plot_learning_curve(rewards, success_rates, params.num_updates, plot_std=True)\n",
    "print(\"Average Reward: {}\".format(np.mean(rewards, axis=0)[-1]))\n",
    "print(\"Average Success Rates: {}\".format(np.mean(success_rates, axis=0)[-1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
