{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All training runs completed!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 18.294957983193278\n",
      "Average Success Rates: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "import utils\n",
    "from utils import show_video\n",
    "from utils import count_model_params\n",
    "from utils import AverageMeter, plot_learning_curve\n",
    "from utils import ParamDict\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "class PPO():\n",
    "\tdef __init__(self, params_in):\n",
    "\t\tself.params = params_in\n",
    "\t\t# env\n",
    "\t\tself.env = self.gen_env()\n",
    "\t\tself.obs_size = self.env.observation_space.shape[0]\n",
    "\t\tself.num_actions = self.env.action_space.n\n",
    "\t\t# rollouts\n",
    "\t\tself.rollouts = self.RolloutStorage(self.params.rollout_size, self.obs_size)\n",
    "\t\t# policy\n",
    "\t\tself.init_policy(self.obs_size, self.num_actions, **self.params.policy_params)\n",
    "\t\n",
    "\tdef init_policy(self, num_inputs, num_actions, hidden_dim, learning_rate, batch_size, policy_epochs,\n",
    "\t\t\t\tentropy_coef=0.001, critic_coef=0.5, clip_param=0.2):\n",
    "\t\tself.actor = self.ActorNetwork(num_inputs, num_actions, hidden_dim)\n",
    "\t\tself.critic = self.CriticNetwork(num_inputs, hidden_dim)\n",
    "\t\tself.critic_coef = critic_coef\n",
    "\t\tself.optimizer = optim.Adam( (list(self.actor.parameters()) + list(self.critic.parameters())), lr=learning_rate)\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.policy_epochs = policy_epochs\n",
    "\t\tself.entropy_coef = entropy_coef\n",
    "\t\tself.clip_param = clip_param\n",
    "\n",
    "\tdef gen_env(self):\n",
    "\t\t# env = Monitor(gym.make(self.params.env_name), './video', force=True)\n",
    "\t\t# env = self.FlatObsWrapperCartPole(gym.make(self.params.env_name))\n",
    "\t\tenv = gym.make(self.params.env_name)\n",
    "\t\treturn env\n",
    "\n",
    "\tdef act(self, state):\n",
    "\t\tlogits = self.actor.forward(state)\n",
    "\t\t# print(\"logits: {}\".format(logits))\n",
    "\t\tdist = Categorical(logits = logits)\n",
    "\t\t# print(\"dist: {}\".format(dist))\n",
    "\t\taction = dist.sample()\n",
    "\t\t# print(\"action: {}\".format(action))\n",
    "\t\tlog_prob = dist.log_prob(action)\n",
    "\t\t# print(\"log_prob: {}\".format(log_prob))\n",
    "\t\treturn action, log_prob\n",
    "\t\n",
    "\tdef update(self, rollouts):\t\n",
    "\t\tfor epoch in range(self.policy_epochs):\n",
    "\t\t\tdata = rollouts.batch_sampler(self.batch_size, get_old_log_probs=True)\n",
    "\t\t\tfor sample in data:\n",
    "\t\t\t\tactions_batch, returns_batch, obs_batch, old_log_probs_batch = sample\n",
    "\t\t\t\tlog_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
    "\t\t\t\t\n",
    "\t\t\t\tvalue_batch = self.critic(obs_batch)\n",
    "\t\t\t\t\n",
    "\t\t\t\tadvantage = returns_batch - value_batch.detach()\n",
    "\t\t\t\told_log_probs_batch = old_log_probs_batch.detach()\n",
    "\n",
    "\t\t\t\tratio = torch.exp(log_probs_batch-old_log_probs_batch)\n",
    "\t\t\t\tsurr1 = ratio * advantage\n",
    "\t\t\t\tsurr2 = torch.clamp(ratio,1-self.clip_param, 1+ self.clip_param) * advantage\n",
    "\n",
    "\t\t\t\tpolicy_loss = -1 * (torch.min(surr1,surr2).mean())\n",
    "\t\t\t\tentropy_loss = -1 * ((entropy_batch).mean())\n",
    "\t\t\t\tloss = torch.nn.MSELoss()\n",
    "\t\t\t\tcritic_loss = loss(returns_batch,value_batch)   \n",
    "\t\t\t\tloss = policy_loss + \\\n",
    "\t\t\t\t\t\tself.critic_coef * critic_loss + \\\n",
    "\t\t\t\t\t\tself.entropy_coef * entropy_loss\n",
    "\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\tloss.backward(retain_graph=False)\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\t\n",
    "\tdef evaluate_actions(self, state, action):\n",
    "\t\tlogits = self.actor.forward(state)\n",
    "\t\tdist = Categorical(logits = logits)\n",
    "\t\tlog_prob = dist.log_prob(action.squeeze(-1)).view(-1, 1)  # ???\n",
    "\t\tentropy = dist.entropy().view(-1, 1)\n",
    "\t\treturn log_prob, entropy\n",
    "\n",
    "\tdef log_policy_rollout(self, policy, env_name, pytorch_policy=False):\n",
    "\t\tenv = Monitor(gym.make(env_name), './video', force=True)\n",
    "\t\t# env = gym.make(env_name)\n",
    "\t\t# env.monitor.start('./video', force=True)\n",
    "\n",
    "\t\tdone = False\n",
    "\t\tepisode_reward = 0\n",
    "\t\tepisode_length = 0\n",
    "\t\tobservation = env.reset()\n",
    "\n",
    "\t\twhile not done:\n",
    "\t\t\tif pytorch_policy: \n",
    "\t\t\t\tobservation = torch.tensor(observation, dtype=torch.float32)\n",
    "\t\t\t\taction = policy.act(observation)[0].data.cpu().numpy()\n",
    "\t\t\telse:\n",
    "\t\t\t\taction = policy.act(observation)[0]\n",
    "\t\t\tobservation, reward, done, info = env.step(action)\n",
    "\n",
    "\t\t\tepisode_reward += reward\n",
    "\t\t\tepisode_length += 1\n",
    "\n",
    "\t\tprint('Total reward:', episode_reward)\n",
    "\t\tprint('Total length:', episode_length)\n",
    "\n",
    "\t\tenv.close()\n",
    "\t\t# env.monitor.close()\n",
    "\t\tshow_video()\n",
    "\t\n",
    "\tdef train(self, seed=123):\n",
    "\t\t# SETTING SEED: it is good practice to set seeds when running experiments to keep results comparable\n",
    "\t\tnp.random.seed(seed)\n",
    "\t\ttorch.manual_seed(seed)\n",
    "\t\tself.env.seed(seed)\n",
    "\n",
    "\t\trollout_time, update_time = AverageMeter(), AverageMeter()  # Loggers\n",
    "\t\trewards, success_rate = [], []\n",
    "\n",
    "\t\tprint(\"Training model with {} parameters...\".format(self.num_params))\n",
    "\n",
    "\t\tfor j in range(self.params.num_updates):\n",
    "\t\t\t## initialization\n",
    "\t\t\tavg_eps_reward, avg_success_rate = AverageMeter(), AverageMeter()\n",
    "\t\t\tdone = False\n",
    "\t\t\tprev_obs = self.env.reset()\n",
    "\t\t\t# print(prev_obs)\n",
    "\t\t\tprev_obs = torch.tensor(prev_obs, dtype=torch.float32)\n",
    "\t\t\teps_reward = 0.\n",
    "\t\t\tstart_time = time.time()\n",
    "\t\t\t\n",
    "\t\t\t## collect rollouts\n",
    "\t\t\tfor step in range(self.rollouts.rollout_size):\n",
    "\t\t\t\tif done:\n",
    "\t\t\t\t\t# store episode statistics\n",
    "\t\t\t\t\tavg_eps_reward.update(eps_reward)\n",
    "\t\t\t\t\tif 'success' in info: \n",
    "\t\t\t\t\t\tavg_success_rate.update(int(info['success']))\n",
    "\n",
    "\t\t\t\t\t# reset Environment\n",
    "\t\t\t\t\tobs = self.env.reset()\n",
    "\t\t\t\t\tobs = torch.tensor(obs, dtype=torch.float32)\n",
    "\t\t\t\t\teps_reward = 0.\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tobs = prev_obs\n",
    "\n",
    "\t\t\t\taction, log_prob = self.act(obs)\n",
    "\t\t\t\tobs, reward, done, info = self.env.step(action.item())\n",
    "\t\t\t\tself.rollouts.insert(step, torch.tensor(done, dtype=torch.float32),\\\n",
    "\t\t\t\t\t\t\t\taction, log_prob, torch.tensor(reward, dtype=torch.float32), prev_obs[0])\n",
    "\t\t\t\t\n",
    "\t\t\t\tprev_obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\t\t\t\teps_reward += reward\n",
    "\t\t\t\n",
    "\t\t\tself.rollouts.compute_returns(self.params.discount)\n",
    "\t\t\trollout_done_time = time.time()\n",
    "\t\t\tself.update(self.rollouts)\n",
    "\n",
    "\t\t\tupdate_done_time = time.time()\n",
    "\t\t\tself.rollouts.reset()\n",
    "\n",
    "\t\t\t## log metrics\n",
    "\t\t\trewards.append(avg_eps_reward.avg)\n",
    "\t\t\tif avg_success_rate.count >= 0:\n",
    "\t\t\t\tsuccess_rate.append(avg_success_rate.avg)\n",
    "\t\t\trollout_time.update(rollout_done_time - start_time)\n",
    "\t\t\tupdate_time.update(update_done_time - rollout_done_time)\n",
    "\t\t\tprint('it {}: avgR: {:.3f} -- rollout_time: {:.3f}sec -- update_time: {:.3f}sec'.format(j, avg_eps_reward.avg, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trollout_time.avg, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tupdate_time.avg))\n",
    "\t\t\tif j % self.params.plotting_iters == 0 and j != 0:\n",
    "\t\t\t\tplot_learning_curve(rewards, success_rate, self.params.num_updates)\n",
    "\t\t\t\t# self.log_policy_rollout(self, self.params.env_name, True)\n",
    "\t\t# render learning logger\n",
    "\t\tclear_output()   # this removes all training outputs to keep the notebook clean\n",
    "\t\t# plot_learning_curve(rewards, success_rate, params.num_updates)\n",
    "\t\t# for _ in range(3):\n",
    "\t\t# \tself.log_policy_rollout(self, self.params.env_name, True)\n",
    "\t\treturn rewards, success_rate\n",
    "\t\t\n",
    "\t@property\n",
    "\tdef num_params(self):\n",
    "\t\treturn count_model_params(self.actor) + count_model_params(self.critic)\n",
    "\t\n",
    "\n",
    "\tclass FlatObsWrapperCartPole(gym.core.ObservationWrapper):\n",
    "\t\t\"\"\"Fully observable gridworld returning a flat grid encoding.\"\"\"\n",
    "\t\tdef __init__(self, env):\n",
    "\t\t\tsuper().__init__(env)\n",
    "\t\t\tself.observation_space = spaces.Box(\n",
    "\t\t\t\tlow=0,\n",
    "\t\t\t\thigh=255,\n",
    "\t\t\t\tshape=((env.observation_space['image'].shape[0]) * (env.observation_space['image'].shape[1]) * 3,),  # number of cells\n",
    "\t\t\t\tdtype='uint8'\n",
    "\t\t\t)\n",
    "\t\t\tself.unwrapped.max_steps = env.max_steps\n",
    "\n",
    "\t\tdef observation(self, obs):\n",
    "\t\t\t# this method is called in the step() function to get the observation\n",
    "\t\t\t# we provide code that gets the grid state and places the agent in it\n",
    "\t\t\tenv = self.unwrapped\n",
    "\t\t\tfull_grid = env.grid.encode()\n",
    "\t\t\t# full_grid[env.agent_pos[0]][env.agent_pos[1]] = np.array([\n",
    "\t\t\t# \tOBJECT_TO_IDX['agent'],\n",
    "\t\t\t# \tCOLOR_TO_IDX['red'],\n",
    "\t\t\t# \tenv.agent_dir\n",
    "\t\t\t# ])\n",
    "\t\t\t# full_grid = full_grid[1:-1, 1:-1]   # remove outer walls of the environment (for efficiency)\n",
    "\t\t\tflattened_grid = full_grid.reshape((np.prod(full_grid.shape),1))\n",
    "\t\t\treturn np.transpose(flattened_grid)\n",
    "\t\t\n",
    "\t\tdef render(self, *args, **kwargs):\n",
    "\t\t\tkwargs['highlight'] = False\n",
    "\t\t\treturn self.unwrapped.render(*args, **kwargs)\n",
    "\n",
    "\tclass ActorNetwork(nn.Module):\n",
    "\t\tdef __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.num_actions = num_actions\n",
    "\t\t\tself.fc = nn.Sequential(nn.Linear(num_inputs,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim, self.num_actions))\n",
    "\t\t\t\t\t\t\t\t\t# no activation in NN, output logit\n",
    "\t\t\n",
    "\t\tdef forward(self, state):\n",
    "\t\t\tx = self.fc(state)\n",
    "\t\t\treturn x\n",
    "\t\n",
    "\tclass CriticNetwork(nn.Module):\n",
    "\t\tdef __init__(self, num_inputs, hidden_dim):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\tself.fc = nn.Sequential(nn.Linear(num_inputs,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim,hidden_dim),\n",
    "\t\t\t\t\t\t\t\t\tnn.Tanh(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(hidden_dim,1))\n",
    "\n",
    "\t\tdef forward(self, state):\n",
    "\t\t\tx = self.fc(state)\n",
    "\t\t\treturn x\n",
    "\t\n",
    "\tclass RolloutStorage():\n",
    "\t\tdef __init__(self, rollout_size, obs_size):\n",
    "\t\t\tself.rollout_size = rollout_size\n",
    "\t\t\tself.obs_size = obs_size\n",
    "\t\t\tself.reset()\n",
    "\n",
    "\t\tdef insert(self, step, done, action, log_prob, reward, obs):\t\n",
    "\t\t\tself.done[step].copy_(done)\n",
    "\t\t\tself.actions[step].copy_(action)\n",
    "\t\t\tself.log_probs[step].copy_(log_prob)\n",
    "\t\t\tself.rewards[step].copy_(reward)\n",
    "\t\t\tself.obs[step].copy_(obs)\n",
    "\n",
    "\t\tdef reset(self):\n",
    "\t\t\tself.done = torch.zeros(self.rollout_size, 1)\n",
    "\t\t\tself.returns = torch.zeros(self.rollout_size+1, 1, requires_grad=False)\n",
    "\t\t\tself.actions = torch.zeros(self.rollout_size, 1, dtype=torch.int64)\n",
    "\t\t\tself.log_probs = torch.zeros(self.rollout_size, 1)\n",
    "\t\t\tself.rewards = torch.zeros(self.rollout_size, 1)\n",
    "\t\t\tself.obs = torch.zeros(self.rollout_size, self.obs_size)\n",
    "\n",
    "\t\tdef compute_returns(self, gamma):\n",
    "\t\t\tself.last_done = (self.done == 1).nonzero().max()\n",
    "\t\t\tself.returns[self.last_done+1] = 0.  # ???\n",
    "\n",
    "\t\t\t#  compute finite-discoiunted return\n",
    "\t\t\tfor step in reversed(range(self.last_done+1)):\n",
    "\t\t\t\tself.returns[step] = self.returns[step + 1] * \\\n",
    "\t\t\t\t\tgamma * (1 - self.done[step]) + self.rewards[step]\n",
    "\n",
    "\t\tdef batch_sampler(self, batch_size, get_old_log_probs=False):\n",
    "\t\t\t# build a generator of single-trajectory: (a, r, s, pi(a, t)) \n",
    "\t\t\tsampler = BatchSampler(\n",
    "\t\t\t\tSubsetRandomSampler(range(self.last_done)),\n",
    "\t\t\t\tbatch_size,\n",
    "\t\t\t\tdrop_last=True)\n",
    "\t\t\tfor indices in sampler:\n",
    "\t\t\t\tif get_old_log_probs:\n",
    "\t\t\t\t\tyield self.actions[indices], self.returns[indices], self.obs[indices], self.log_probs[indices]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tyield self.actions[indices], self.returns[indices], self.obs[indices]\n",
    "\n",
    "# Test Trian\n",
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "\t# policy_class = PPO,  # Policy class to use (replaced later)\n",
    "\thidden_dim = 32,\t\t  # dimension of the hidden state in actor network\n",
    "\tlearning_rate = 1e-3,\t # learning rate of policy update\n",
    "\tbatch_size = 1024,\t\t# batch size for policy update\n",
    "\tpolicy_epochs = 4,\t\t# number of epochs per policy update\n",
    "\tentropy_coef = 0.001,\t # hyperparameter to vary the contribution of entropy loss\n",
    "\tcritic_coef = 0.5\t\t # Coefficient of critic loss when weighted against actor loss\n",
    ")\n",
    "params = ParamDict(\n",
    "\tpolicy_params = policy_params,\n",
    "\trollout_size = 2050,\t  # number of collected rollout steps per policy update\n",
    "\tnum_updates = 15,\t\t# number of training policy iterations\n",
    "\tdiscount = 0.99,\t\t  # discount factor\n",
    "\tplotting_iters = 25,\t  # interval for logging graphs and policy rollouts\n",
    "\tenv_name = 'CartPole-v1',  # we are using a tiny environment here for testing\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ppo = PPO(params)\n",
    "# ppo.train()\n",
    "# print(\"Training completed!\")\n",
    "\n",
    "n_seeds = 2\n",
    "rewards, success_rates = [], []\n",
    "for i in range(n_seeds):\n",
    "\tppo = PPO(params)\n",
    "\tprint(\"Start training run {}!\".format(i))\n",
    "\tr, sr = ppo.train(i)\n",
    "\trewards.append(r); success_rates.append(sr)\n",
    "print('All training runs completed!')\n",
    "\n",
    "plot_learning_curve(rewards, success_rates, params.num_updates, plot_std=True)\n",
    "print(\"Average Reward: {}\".format(np.mean(rewards, axis=0)[-1]))\n",
    "print(\"Average Success Rates: {}\".format(np.mean(success_rates, axis=0)[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 18.294957983193278\n",
      "Average Success Rates: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[18.581818181818182,\n",
       "  20.896907216494846,\n",
       "  23.77906976744186,\n",
       "  20.663265306122447,\n",
       "  22.0,\n",
       "  24.228915662650603,\n",
       "  19.99009900990099,\n",
       "  21.73404255319149,\n",
       "  20.43,\n",
       "  22.96590909090909,\n",
       "  21.69148936170213,\n",
       "  19.34285714285714,\n",
       "  18.925925925925927,\n",
       "  22.197802197802197,\n",
       "  18.554545454545455,\n",
       "  22.47252747252747,\n",
       "  20.636363636363637,\n",
       "  20.505050505050505,\n",
       "  19.825242718446603,\n",
       "  19.37142857142857],\n",
       " [19.601941747572816,\n",
       "  20.4,\n",
       "  20.646464646464647,\n",
       "  19.20754716981132,\n",
       "  18.743119266055047,\n",
       "  19.05607476635514,\n",
       "  18.944444444444443,\n",
       "  19.88235294117647,\n",
       "  18.65137614678899,\n",
       "  19.409523809523808,\n",
       "  19.567307692307693,\n",
       "  18.381818181818183,\n",
       "  18.97222222222222,\n",
       "  18.088495575221238,\n",
       "  18.169642857142858,\n",
       "  17.95614035087719,\n",
       "  18.59090909090909,\n",
       "  18.44144144144144,\n",
       "  18.5,\n",
       "  17.218487394957982]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(rewards, success_rates, params.num_updates, plot_std=True)\n",
    "print(\"Average Reward: {}\".format(np.mean(rewards, axis=0)[-1]))\n",
    "print(\"Average Success Rates: {}\".format(np.mean(success_rates, axis=0)[-1]))\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_learning_curve_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ec3ab522166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# plot_learning_curve(rewards, success_rates, params.num_updates, plot_std=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mplot_learning_curve_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Success Rates: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccess_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_learning_curve_mod' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_std_learning_curves_mod(rewards, success_rates, num_it, no_show=False):\n",
    "    r, sr = np.asarray(rewards), np.asarray(success_rates)\n",
    "    df = pd.DataFrame(r).melt()\n",
    "    sns.lineplot(x=\"variable\", y=\"value\", data=df, label='reward/eps')\n",
    "    df = pd.DataFrame(sr).melt()\n",
    "    sns.lineplot(x=\"variable\", y=\"value\", data=df, label='success rate')\n",
    "    plt.xlabel(\"Training iterations\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlim([0, num_it])\n",
    "#     plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "    if not no_show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_learning_curve(rewards, success_rate, num_it, plot_std=False):\n",
    "    if plot_std:\n",
    "        # plots shaded regions if list of reward timeseries is given\n",
    "#         plot_std_learning_curves(rewards, success_rate, num_it)\n",
    "        pass\n",
    "    else:\n",
    "        plt.plot(rewards, label='reward/eps')\n",
    "        if success_rate:\n",
    "            plt.plot(success_rate, label='success rate')\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.ylabel('return / eps')\n",
    "        plt.ylim([0, 1])\n",
    "        plt.xlim([0, num_it - 1])\n",
    "        plt.xlabel('train iter')\n",
    "        plt.grid('on')\n",
    "        plt.show()\n",
    "\n",
    "# plot_learning_curve(rewards, success_rates, params.num_updates, plot_std=True)\n",
    "\n",
    "plot_learning_curve_mod(rewards, success_rates, params.num_updates, plot_std=True)\n",
    "print(\"Average Reward: {}\".format(np.mean(rewards, axis=0)[-1]))\n",
    "print(\"Average Success Rates: {}\".format(np.mean(success_rates, axis=0)[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
